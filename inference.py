import argparse
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import pickle
from pathlib import Path
import warnings
import sys
import os

# æ·»åŠ ai_serviceæ¨¡å—è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

# ä»ai_serviceå¯¼å…¥å¿…è¦çš„å‡½æ•°
from ai_service import (
    convert_csv_to_virtual_features,
    perform_inference_from_csvs,
    kelong_model,
    kelong_scaler,
    kelong_label_map,
    processed_model,
    processed_scaler,
    processed_feature_cols
)

# å¿½ç•¥ä¸€äº›ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore")

# ===========================================================
# 1. é…ç½®åŒºåŸŸ (10ä¸ªåŸºç¡€ç‰¹å¾)
# ===========================================================
READ_COLS = [
    "v_pitch",
    "v_roll",
    "v_yaw_rate",
    "v_linear_acc_x",
    "v_linear_acc_y",
    "v_linear_acc_z",
    "v_z_highpass",
    "v_jerk_x",
    "v_jerk_y",
    "v_jerk_z",
]
FEATURE_COLS = READ_COLS + ["v_acc_mag"]

# ===========================================================
# 2. æ¨¡å‹å®šä¹‰
# ===========================================================
class SEBlock(nn.Module):
    def __init__(self, channels: int, reduction: int = 16):
        super().__init__()
        hidden = max(1, channels // reduction)
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Sequential(
            nn.Conv1d(channels, hidden, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv1d(hidden, channels, kernel_size=1),
            nn.Sigmoid(),
        )
    def forward(self, x):
        return x * self.fc(self.pool(x))

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dropout=0.0):
        super().__init__()
        padding = kernel_size // 2
        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)
        self.bn1 = nn.BatchNorm1d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.dropout = nn.Dropout(dropout)
        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=False)
        self.bn2 = nn.BatchNorm1d(out_channels)
        self.se = SEBlock(out_channels)
        
        self.shortcut = nn.Identity()
        if in_channels != out_channels or stride != 1:
            self.shortcut = nn.Sequential(
                nn.Conv1d(in_channels, out_channels, 1, stride, bias=False),
                nn.BatchNorm1d(out_channels)
            )

    def forward(self, x):
        ident = self.shortcut(x)
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.dropout(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.se(out)
        return self.relu(out + ident)

class ResNet1D(nn.Module):
    def __init__(self, in_channels, num_classes, base_channels=128, dropout=0.2):
        super().__init__()
        self.stem = nn.Sequential(
            nn.Conv1d(in_channels, base_channels, 7, padding=3, bias=False),
            nn.BatchNorm1d(base_channels),
            nn.ReLU(inplace=True)
        )
        
        self.layer1 = self._make_layer(base_channels, base_channels, blocks=2, kernel_size=7, stride=1, dropout=dropout)
        self.layer2 = self._make_layer(base_channels, base_channels * 2, blocks=2, kernel_size=5, stride=2, dropout=dropout)
        self.layer3 = self._make_layer(base_channels * 2, base_channels * 4, blocks=2, kernel_size=3, stride=2, dropout=dropout)

        self.pool = nn.AdaptiveAvgPool1d(1)
        self.classifier = nn.Sequential(
            nn.Dropout(p=dropout),
            nn.Linear(base_channels * 4, num_classes),
        )

    def _make_layer(self, in_channels, out_channels, blocks, kernel_size, stride, dropout):
        layers = [
            ResidualBlock(in_channels, out_channels, kernel_size=kernel_size, stride=stride, dropout=dropout),
        ]
        for _ in range(1, blocks):
            layers.append(ResidualBlock(out_channels, out_channels, kernel_size=kernel_size, stride=1, dropout=dropout))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.stem(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.pool(x).squeeze(-1)
        x = self.classifier(x)
        return x

# ===========================================================
# 3. æ•°æ®é¢„å¤„ç†
# ===========================================================
def preprocess_excel(file_path, scaler):
    try:
        if str(file_path).lower().endswith(".csv"):
            df = pd.read_csv(file_path)
        else:
            df = pd.read_excel(file_path)
    except Exception:
        return None

    missing = [c for c in READ_COLS if c not in df.columns]
    if missing:
        return None

    df_subset = df[READ_COLS].copy()
    for c in READ_COLS:
        df_subset[c] = pd.to_numeric(df_subset[c], errors="coerce")
    
    if df_subset.isna().any().any():
        df_subset = df_subset.interpolate(method='linear').ffill().bfill()
        df_subset = df_subset.fillna(0)

    acc_x = df_subset["v_linear_acc_x"].to_numpy(dtype=np.float32)
    acc_y = df_subset["v_linear_acc_y"].to_numpy(dtype=np.float32)
    acc_z = df_subset["v_linear_acc_z"].to_numpy(dtype=np.float32)
    acc_mag = np.sqrt(acc_x**2 + acc_y**2 + acc_z**2).reshape(-1, 1)

    data = df_subset.to_numpy(dtype=np.float32)
    data = np.concatenate([data, acc_mag], axis=1)

    try:
        data_scaled = scaler.transform(data)
    except Exception:
        return None
    
    return data_scaled

# ===========================================================
# 4. ä¸»ç¨‹åº
# ===========================================================
def main():
    parser = argparse.ArgumentParser(description="çŒ«è¡Œä¸ºè¯†åˆ« - åŒæ¨¡å‹ç‰ˆæœ¬")
    parser.add_argument("--input", type=str, required=True, help="åŸå§‹IMUæ•°æ®CSVæ–‡ä»¶è·¯å¾„ï¼ˆæ ¼å¼ï¼šsequence, timestamp, acc_x, acc_y, acc_z, gyro_x, gyro_y, gyro_zï¼‰")
    parser.add_argument("--output_dir", type=str, default=None, help="è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼Œç”¨äºä¿å­˜ç”Ÿæˆçš„ç‰¹å¾æ–‡ä»¶ï¼‰")
    args = parser.parse_args()

    input_path = Path(args.input)
    if not input_path.exists():
        print(f"âŒ Error: æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
    if kelong_model is None and processed_model is None:
        print("âŒ Error: æ¨¡å‹æœªåŠ è½½ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶è·¯å¾„")
        return
    
    print(f"ğŸ“‚ è¯»å–æ–‡ä»¶: {input_path}")
    
    # --- A. ä»CSVæ–‡ä»¶è¯»å–å¹¶è½¬æ¢ä¸ºè™šæ‹Ÿç‰¹å¾ ---
    try:
        timestamp_df, window_df = convert_csv_to_virtual_features(input_path)
        print(f"âœ… æˆåŠŸç”Ÿæˆç‰¹å¾:")
        print(f"   - per_timestampç‰¹å¾: {len(timestamp_df)} è¡Œ")
        print(f"   - per_windowç‰¹å¾: {len(window_df)} è¡Œ")
    except Exception as e:
        print(f"âŒ Error: ç‰¹å¾è½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # --- B. ä¿å­˜ç‰¹å¾æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰---
    if args.output_dir:
        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        base_name = input_path.stem
        timestamp_csv = output_dir / f"{base_name}_virtual_per_timestamp_feature.csv"
        window_csv = output_dir / f"{base_name}_virtual_per_window_feature.csv"
        
        timestamp_df.to_csv(timestamp_csv, index=False)
        window_df.to_csv(window_csv, index=False)
        print(f"ğŸ’¾ ç‰¹å¾æ–‡ä»¶å·²ä¿å­˜:")
        print(f"   - {timestamp_csv}")
        print(f"   - {window_csv}")
    else:
        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
        import tempfile
        temp_dir = Path(tempfile.gettempdir())
        base_name = input_path.stem
        timestamp_csv = temp_dir / f"{base_name}_virtual_per_timestamp_feature.csv"
        window_csv = temp_dir / f"{base_name}_virtual_per_window_feature.csv"
        
        timestamp_df.to_csv(timestamp_csv, index=False)
        window_df.to_csv(window_csv, index=False)
    
    # --- C. æ‰§è¡Œæ¨ç† ---
    print("\nğŸ” æ‰§è¡Œæ¨ç†...")
    try:
        inference_result = perform_inference_from_csvs(timestamp_csv, window_csv)
        
        if inference_result.get("status") == "error":
            print(f"âŒ Error: {inference_result.get('error', 'Unknown error')}")
            return
        
        action = inference_result.get("action", "unknown")
        confidence = inference_result.get("confidence", 0.0)
        model_used = inference_result.get("model_used", "Unknown")
        
        # --- D. è¾“å‡ºç»“æœ ---
        print("\n" + "="*50)
        print(f"ğŸ± è¯†åˆ«ç»“æœ: ã€ {action} ã€‘")
        print(f"ğŸ¯ ç½®ä¿¡åº¦:   {confidence:.2%}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_used}")
        print("="*50 + "\n")
        
    except Exception as e:
        print(f"âŒ Error: æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆå¦‚æœä¸æ˜¯ç”¨æˆ·æŒ‡å®šçš„è¾“å‡ºç›®å½•ï¼‰
        if not args.output_dir:
            try:
                if timestamp_csv.exists():
                    timestamp_csv.unlink()
                if window_csv.exists():
                    window_csv.unlink()
            except:
                pass

if __name__ == "__main__":
    main()